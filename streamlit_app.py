# streamlit_app.py

import streamlit as st
import torch
from transformers import BertTokenizer, BertForSequenceClassification
import joblib

# âœ… ë¼ë²¨ ì¸ì½”ë” ë¡œë“œ
category_encoder = joblib.load("category_encoder.pkl")
target_encoder = joblib.load("target_encoder.pkl")

# âœ… í† í¬ë‚˜ì´ì € & ëª¨ë¸ ë¡œë“œ
tokenizer = BertTokenizer.from_pretrained("klue/bert-base")

# torch.loadë¡œ ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ
model = BertForSequenceClassification.from_pretrained("klue/bert-base", num_labels=len(category_encoder.classes_) + len(target_encoder.classes_))
model.load_state_dict(torch.load("hate_speech_model.pt", map_location='cpu'))
model.eval()

st.title("ğŸ—¨ï¸ í˜ì˜¤ í‘œí˜„ íƒì§€ê¸°")
st.write("ì˜¨ë¼ì¸ ì»¤ë®¤ë‹ˆí‹°ë‚˜ ì†Œì…œë¯¸ë””ì–´ì—ì„œ ì ‘í•œ í˜ì˜¤ í‘œí˜„ì´ ì˜ì‹¬ë˜ëŠ” ë¬¸ì¥ì„ ì…ë ¥í•´ë³´ì„¸ìš”!")

text = st.text_input("ë¬¸ì¥ì„ ì…ë ¥í•˜ì„¸ìš”:")

if st.button("ë¶„ì„í•˜ê¸°"):
    if text.strip() == "":
        st.warning("ë¬¸ì¥ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:
        # ì…ë ¥ í† í¬ë‚˜ì´ì§•
        inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
        
        with torch.no_grad():
            outputs = model(**inputs)
            logits = outputs.logits

            # âœ¨ logitsê°€ ë‹¤ì¤‘ ì¶œë ¥ì´ë©´ category, targetìœ¼ë¡œ split
            # ì˜ˆì‹œ: [batch_size, total_labels] -> ìŠ¬ë¼ì´ìŠ¤ í•„ìš”
            # ì˜ˆ: ì•ìª½ì€ category, ë’¤ìª½ì€ target
            category_logits = logits[:, :len(category_encoder.classes_)]
            target_logits = logits[:, len(category_encoder.classes_):]

            pred_category_idx = torch.argmax(category_logits, dim=1).item()
            pred_target_idx = torch.argmax(target_logits, dim=1).item()

            pred_category = category_encoder.inverse_transform([pred_category_idx])[0]
            pred_target = target_encoder.inverse_transform([pred_target_idx])[0]

        st.success(f"âœ… ì˜ˆì¸¡ëœ í˜ì˜¤ í‘œí˜„ ì¹´í…Œê³ ë¦¬: **{pred_category}**")
        st.success(f"âœ… ì˜ˆì¸¡ëœ í˜ì˜¤ í‘œí˜„ ëŒ€ìƒ: **{pred_target}**")